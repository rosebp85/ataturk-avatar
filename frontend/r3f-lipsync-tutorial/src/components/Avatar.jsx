/*
Auto-generated by: https://github.com/pmndrs/gltfjsx
Command: npx gltfjsx@6.2.3 public/models/646d9dcdc8a5f5bddbfac913.glb -o src/components/Avatar.jsx -r public
*/

import { useAnimations, useFBX, useGLTF } from "@react-three/drei";   // Importing hooks from drei for loading GLTF and FBX models
import { useFrame, useLoader } from "@react-three/fiber";             // Importing hooks from react-three-fiber for animation and loading
import { useControls } from "leva";                                   // Importing Leva for UI controls
import React, { useEffect, useMemo, useRef, useState } from "react";  // Importing React and hooks for component logic

import * as THREE from "three";                                       // Importing THREE.js for 3D rendering and manipulation

// This object maps phoneme keys (e.g., "A", "B", "C") to corresponding mouth shapes or visemes
// These mappings are used to synchronize the 3D model's mouth movements with audio cues
const corresponding = {
    "X": "rest",
    "A": "FF",
    "B": "PP",
    "C": "kk",
    "D": "DD",
    "E": "ih",
    "F": "aa",
    "G": "oo",
    "H": "oh"
};


// Avatar component with controls for head movement and morph target smoothing
export function Avatar({ audioUrl, jsonUrl, ...props }) {
  const {
    headFollow,
    smoothMorphTarget,
    morphTargetSmoothing,
  } = useControls({
    headFollow: true,
    smoothMorphTarget: true,
    morphTargetSmoothing: 0.5,
  });
  
  // Load and parse audio and JSON data for lipsync functionality
  const audio = useMemo(() => new Audio(audioUrl), [audioUrl]);
  const jsonFile = useLoader(THREE.FileLoader, jsonUrl);
  const lipsync = useMemo(() => {
    try {
      if (!jsonFile || jsonFile.startsWith("<!DOCTYPE html>")) {
        throw new Error("Invalid JSON file. Check the jsonUrl or server response.");
      }
      return JSON.parse(jsonFile); // Parse JSON file
    } catch (error) {
      console.error("Failed to parse JSON file:", error, jsonFile);
      return { mouthCues: [] }; // Fallback to an empty object
    }
  }, [jsonFile]);


  // Effect to check and log the status of the loaded JSON file
  useEffect(() => {
    if (!jsonFile) {
      console.error("JSON file not loaded. Check the jsonUrl:", jsonUrl);
    } else {
      console.log("Loaded JSON file:", jsonFile);
    }
  }, [jsonFile, jsonUrl]);


  // Log the jsonUrl for debugging
  useEffect(() => {
    console.log("JSON URL:", jsonUrl);
  }, [jsonUrl]);


  // useFrame hook to synchronize the 3D model's mouth movements with audio playback
  // It updates the morph target influences based on the current audio time and mouth cues
  useFrame(() => {
    const currentAudioTime = audio.currentTime;
    if (audio.paused || audio.ended) {
      setAnimation("Idle");
      return;
    }    
    // Gradually reset all viseme influences to 0 for smooth transitions
      Object.values(corresponding).forEach((viseme) => {
        if (nodes.AvatarHead && nodes.AvatarHead.morphTargetDictionary[viseme] !== undefined) {
          nodes.AvatarHead.morphTargetInfluences[nodes.AvatarHead.morphTargetDictionary[viseme]] = THREE.MathUtils.lerp(
            nodes.AvatarHead.morphTargetInfluences[nodes.AvatarHead.morphTargetDictionary[viseme]],
            0, 
            morphTargetSmoothing
          );
        }
        
        if (nodes.AvatarTeeth && nodes.AvatarTeeth.morphTargetDictionary[viseme] !== undefined) {
          nodes.AvatarTeeth.morphTargetInfluences[nodes.AvatarTeeth.morphTargetDictionary[viseme]] = THREE.MathUtils.lerp(
            nodes.AvatarTeeth.morphTargetInfluences[nodes.AvatarTeeth.morphTargetDictionary[viseme]],
            0, 
            morphTargetSmoothing
          );
        }
    });
  
    // Iterate through the mouth cues and apply the corresponding viseme influences
    for (let i = 0; i < lipsync.mouthCues.length; i++) {
      const mouthCue = lipsync.mouthCues[i];
      if (currentAudioTime >= mouthCue.start && currentAudioTime <= mouthCue.end) {
        const viseme = corresponding[mouthCue.value]; 
        if (!viseme) break; 
  
        // Apply the viseme influence to the AvatarHead
        if (nodes.AvatarHead && nodes.AvatarHead.morphTargetDictionary[viseme] !== undefined) {
          nodes.AvatarHead.morphTargetInfluences[nodes.AvatarHead.morphTargetDictionary[viseme]] = THREE.MathUtils.lerp(
            nodes.AvatarHead.morphTargetInfluences[nodes.AvatarHead.morphTargetDictionary[viseme]],
            1, 
            morphTargetSmoothing
          );
        }
  
        // Apply the viseme influence to the AvatarTeeth
        if (nodes.AvatarTeeth && nodes.AvatarTeeth.morphTargetDictionary[viseme] !== undefined) {
          nodes.AvatarTeeth.morphTargetInfluences[nodes.AvatarTeeth.morphTargetDictionary[viseme]] = THREE.MathUtils.lerp(
            nodes.AvatarTeeth.morphTargetInfluences[nodes.AvatarTeeth.morphTargetDictionary[viseme]],
            1, 
            morphTargetSmoothing
          );
        }
  
        break; 
      }
    }
  });

  // Effect to handle audio playback when the audioUrl changes
  useEffect(() => {
    if (audioUrl && audioRef.current) {
      const audio = audioRef.current;
      audio.load();
      audio.play().catch(err => {
        console.warn("Audio playback failed due to browser restrictions.", err);
      });
    }
  }, [audioUrl]);

  const audioRef = useRef();

  // Effect to handle audio loading and playback when the audioUrl changes
  // It sets up error handling for audio loading and playback
  // It also plays the audio automatically when the audioUrl changes
  useEffect(() => {
    if (audioUrl) {
      audio.src = audioUrl; 
      audio.load();
      
      audio.addEventListener("error", (e) => {
        console.error("Audio failed to load or play. Check the audioUrl:", audioUrl, e);
      });
      
      audio.play().catch((err) => {
        console.warn("Audio playback failed. Possible unsupported format or browser restriction:", err);
      });
    } else {
      console.error("No audioUrl provided.");
    }
  }, [audioUrl]);

  // Load the GLTF model and FBX animation
  const { nodes, materials } = useGLTF('/models/model.glb');
  const { animations: idleAnimation } = useFBX("/animations/Idle.fbx");
  idleAnimation[0].name = "Idle";
  const [animation, setAnimation] = useState("Idle");

  // Set up animation controls using useAnimations hook
  const group = useRef();
  const { actions } = useAnimations(
    [idleAnimation[0]],
    group
  );

  // Effect to handle animation playback based on the selected animation
  useEffect(() => {
    actions[animation].reset().fadeIn(0.5).play();
    return () => actions[animation].fadeOut(0.5);
  }, [animation]);
  

  // Effect to handle head movement based on the headFollow state
  useFrame((state) => {
    if (headFollow) {
      group.current.getObjectByName("Head").lookAt(state.camera.position);
    }
  });
  
  // Effect to log the mouth cues and audio time for debugging
  // It logs the mouth cues and audio time whenever they change
  useEffect(() => {
    console.log("Mouth Cues:", lipsync.mouthCues);
    console.log("Audio Time:", audio.currentTime);
    console.log("Morph Target Dictionary:", nodes.AvatarHead?.morphTargetDictionary);
  }, [lipsync, audio, nodes]);
  
  // Effect to log the loaded model and its components for debugging
  useEffect(() => {
    group.current.traverse((child) => {
      if (child.isMesh) {
        console.log("Mesh found:", child.name);
      }
    });
  }, []);
  
  // Effect to log the visibility of all meshes in the model for debugging
  // It logs the visibility of each mesh in the model
  useEffect(() => {
    group.current.traverse((child) => {
      if (child.isMesh) {
        console.log("Mesh found:", child.name, "Visible:", child.visible);
      }
    });
  }, []);

  // Effect to log the names and positions of all meshes in the model for debugging
  useEffect(() => {
    group.current.traverse((child) => {
      if (child.isMesh) {
        console.log("Mesh found:", child.name, "Position:", child.position);
      }
    });
  }, []);

  // Effect to log the materials of all meshes in the model for debugging
  useEffect(() => {
    group.current.traverse((child) => {
      if (child.isMesh) {
        console.log("Mesh found:", child.name, "Material:", child.material);
      }
    });
  }, []);

  // Effect to log the skeleton of all meshes in the model for debugging
  return (
    <group {...props} dispose={null} ref={group}>
      <primitive object={nodes.AvatarRoot} />
  
      {nodes.AvatarHead && (
        <skinnedMesh
          name="AvatarHead"
          geometry={nodes.AvatarHead.geometry}
          
   
        />
      )}
  
      {nodes.AvatarHair && (
        <skinnedMesh
          geometry={nodes.AvatarHair.geometry}
          material={materials.AvatarHair}
          skeleton={nodes.AvatarHair.skeleton}
          visible={false} 
        />
      )}
  
      {nodes.AvatarTeeth && (
        <skinnedMesh
          name="AvatarTeeth"
          geometry={nodes.AvatarTeeth.geometry}
          material={materials.AvatarTeeth}
          skeleton={nodes.AvatarTeeth.skeleton}
          morphTargetDictionary={nodes.AvatarTeeth.morphTargetDictionary}
          morphTargetInfluences={nodes.AvatarTeeth.morphTargetInfluences}
          visible={true}
        />
      )}
  
      {nodes.outfit && (
        <skinnedMesh
          geometry={nodes.outfit.geometry}
          material={materials.outfit}
          skeleton={nodes.outfit.skeleton}
          visible={false} 
        />
      )}
  
      {nodes.EyeLeft && nodes.EyeRight && (
        <>
          <skinnedMesh
            name="EyeLeft"
            geometry={nodes.EyeLeft.geometry}
            material={materials.AvatarEye}
            skeleton={nodes.EyeLeft.skeleton}
            morphTargetDictionary={nodes.EyeLeft.morphTargetDictionary}
            morphTargetInfluences={nodes.EyeLeft.morphTargetInfluences}
          />
          <skinnedMesh
            name="EyeRight"
            geometry={nodes.EyeRight.geometry}
            material={materials.AvatarEye}
            skeleton={nodes.EyeRight.skeleton}
            morphTargetDictionary={nodes.EyeRight.morphTargetDictionary}
            morphTargetInfluences={nodes.EyeRight.morphTargetInfluences}
          />
        </>
      )}
  
      {nodes.LeftHand && (
        <skinnedMesh
          name="LeftHand"
          geometry={nodes.LeftHand.geometry}
          material={materials.AvatarSkin}
          skeleton={nodes.LeftHand.skeleton}
        />
      )}
  
      {nodes.RightHand && (
        <skinnedMesh
          name="RightHand"
          geometry={nodes.RightHand.geometry}
          material={materials.AvatarSkin}
          skeleton={nodes.RightHand.skeleton}
        />
      )}
    </group>
  );
}

// Preload the GLTF model to improve performance by loading it in advance
useGLTF.preload("/models/model.glb");